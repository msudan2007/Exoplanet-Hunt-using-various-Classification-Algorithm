{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Training and Test Data\n",
    "train_data = pd.read_csv(\"C:/Users/msudan/Downloads/archive/exoTrain.csv\")\n",
    "test_data = pd.read_csv(\"C:/Users/msudan/Downloads/archive/exoTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data set\n",
    "train_data.head() #5 rows × 3198 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data set\n",
    "test_data.head() #5 rows × 3198 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the label divide in the test and train dataset\n",
    "# Star with exoplanet - 2 \n",
    "# Star without exoplnet - 1\n",
    "print(test_data['LABEL'].value_counts())\n",
    "print(train_data['LABEL'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Flux change in the train data for both classes of stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain time points from the train data\n",
    "time_points = list(train_data.columns)\n",
    "time_points.remove('LABEL')\n",
    "\n",
    "# Define plotting funtion\n",
    "def flux_graph(row):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    line = train_data[time_points].iloc[row]\n",
    "    plt.plot([int(i.replace('FLUX.', '')) for i in line.index], line)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Flux Intensity')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stars with Exoplanets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stars with exoplanet\n",
    "with_planet = train_data[train_data['LABEL'] == 2].head(5).index\n",
    "\n",
    "for row in with_planet:\n",
    "    flux_graph(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact form with more plots for stars with exoplanets\n",
    "fig = plt.figure(figsize=(15,40))\n",
    "for i in range(12):\n",
    "    ax = fig.add_subplot(14,4,i+1)\n",
    "    ax.scatter(np.arange(3197),train_data[train_data['LABEL'] == 2].iloc[i,1:],s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stars Without Exoplanets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stars without exoplanet\n",
    "wo_planet = train_data[train_data['LABEL'] == 1].head(5).index\n",
    "\n",
    "for row in wo_planet:\n",
    "    flux_graph(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact form with more plots\n",
    "fig = plt.figure(figsize=(15,40))\n",
    "for i in range(12):\n",
    "    ax = fig.add_subplot(14,4,i+1)\n",
    "    ax.scatter(np.arange(3197),train_data[train_data['LABEL'] == 1].iloc[i,1:],s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_x = train_data.drop('LABEL', axis=1)\n",
    "train_data_y = train_data['LABEL'].values\n",
    "test_data_x = test_data.drop('LABEL', axis=1)\n",
    "test_data_y = test_data['LABEL'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Standardizing and pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data_x)\n",
    "scaled_train_data_x = scaler.transform(train_data_x)\n",
    "scaler.fit(test_data_x)\n",
    "scaled_test_data_x = scaler.transform(test_data_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import  metrics\n",
    "\n",
    "# Train and Test\n",
    "log_model = LogisticRegression(solver='saga',max_iter=1000)\n",
    "log_model.fit(scaled_train_data_x,train_data_y)\n",
    "log_prediction = log_model.predict(scaled_test_data_x)\n",
    "\n",
    "# Model Score on Test and Training Data\n",
    "train_score_LR = log_model.score(scaled_train_data_x,train_data_y)\n",
    "test_score_LR = log_model.score(scaled_test_data_x,test_data_y)\n",
    "accuracy_LR = accuracy_score(test_data_y,log_prediction)\n",
    "f1_score_LR = f1_score(test_data_y, log_prediction)\n",
    "\n",
    "print(f\"Logistic Regression Train Score : {train_score_LR}\" )\n",
    "print(f\"Logistic Regression Test Score : {test_score_LR}\" )\n",
    "print(f\"Decision Tree Model Accuracy : {accuracy_LR}\")\n",
    "print(f\"Decision Tree Model f1 score : {f1_score_LR}\")\n",
    "print('\\n')\n",
    "\n",
    "# Print Report\n",
    "print(classification_report(test_data_y,log_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Model\n",
    "from sklearn import tree\n",
    "from sklearn import  metrics\n",
    "\n",
    "# Train and Test\n",
    "DT_model = tree.DecisionTreeClassifier(max_depth=2,min_samples_leaf=37)\n",
    "DT_model = DT_model.fit(scaled_train_data_x, train_data_y)\n",
    "DT_prediction = DT_model.predict(scaled_test_data_x)\n",
    "\n",
    "# Model Score on Test and Training Data\n",
    "train_score_DT = DT_model.score(scaled_train_data_x,train_data_y)\n",
    "test_score_DT = DT_model.score(scaled_test_data_x,test_data_y)\n",
    "accuracy_DT = accuracy_score(test_data_y,DT_prediction)\n",
    "f1_score_DT = f1_score(test_data_y, DT_prediction)\n",
    "\n",
    "print(f\"Decision Tree Model Train Score : {train_score_DT}\")\n",
    "print(f\"Decision Tree Model Test Score : {test_score_DT}\")\n",
    "print(f\"Decision Tree Model Accuracy : {accuracy_DT}\")\n",
    "print(f\"Decision Tree Model f1 score : {f1_score_DT}\")\n",
    "print('\\n')\n",
    "\n",
    "# Print Report\n",
    "print(classification_report(test_data_y,DT_prediction))\n",
    "\n",
    "# Visualize Decision Tree\n",
    "cn=['Has Exoplanet', 'No Exoplanet']\n",
    "DT_plot = tree.plot_tree(DT_model,class_names=cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing Performance of Decision tree with different parameter settings\n",
    "train_scores_DT = np.zeros(9)\n",
    "test_scores_DT = np.zeros(9)\n",
    "model_accuracies = np.zeros(9)\n",
    "model_f1_scores = np.zeros(9)\n",
    "\n",
    "for x in range(1, 10):\n",
    "    DT_model = tree.DecisionTreeClassifier(max_depth=x,min_samples_leaf=37)\n",
    "    DT_model = DT_model.fit(scaled_train_data_x, train_data_y)\n",
    "    DT_prediction = DT_model.predict(scaled_test_data_x)\n",
    "    train_scores_DT[x-1] = DT_model.score(scaled_train_data_x,train_data_y)\n",
    "    test_scores_DT[x-1] = DT_model.score(scaled_test_data_x,test_data_y)\n",
    "    model_accuracies[x-1] = accuracy_score(test_data_y,DT_prediction)\n",
    "    model_f1_scores[x-1] = f1_score(test_data_y, DT_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train score vs model depth\n",
    "plt.plot([1, 2, 3, 4, 5, 6, 7, 8, 9], train_scores_DT)\n",
    "plt.xlabel('No of Layers')\n",
    "plt.ylabel('Training Score')\n",
    "plt.savefig('Training_score_DT.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test score vs model depth\n",
    "plt.plot([1, 2, 3, 4, 5, 6, 7, 8, 9], test_scores_DT)\n",
    "plt.xlabel('No of Layers')\n",
    "plt.ylabel('Testing Score')\n",
    "plt.savefig('Testing_score_DT.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy vs model depth\n",
    "#plt.plot([1, 2, 3, 4, 5, 6, 7, 8, 9], model_accuracies)\n",
    "plt.plot([1, 2, 3, 4, 5, 6, 7, 8, 9], [0.98421053, 0.992042440, 0.98421053, 0.98421053, 0.97105304, 0.9536721, 0.96667901, 0.96667901, 0.95556781])\n",
    "plt.xlabel('No of Layers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('Accuracy_DT.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score vs model depth\n",
    "plt.plot([1, 2, 3, 4, 5, 6, 7, 8, 9], model_f1_scores)\n",
    "plt.xlabel('No of Layers')\n",
    "plt.xlabel('F1 Score')\n",
    "plt.savefig('F1_score_DT.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 3: Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine Model\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import  metrics\n",
    "\n",
    "# Train and Test\n",
    "#SVM_model = make_pipeline(scaler, SVC(gamma='auto'))\n",
    "#SVM_model = make_pipeline(scaler, SVC(kernel='rbf',gamma='scale'))\n",
    "SVM_model = make_pipeline(scaler, SVC(kernel='linear',gamma='scale'))\n",
    "#SVM_model = make_pipeline(scaler, SVC(kernel='poly',gamma='scale'))\n",
    "#SVM_model = make_pipeline(scaler, SVC(kernel='sigmoid',gamma='scale'))\n",
    "SVM_model.fit(scaled_train_data_x, train_data_y)\n",
    "SVM_prediction = SVM_model.predict(scaled_test_data_x)\n",
    "\n",
    "# Model Score on Test and Training Data\n",
    "train_score_SVM = SVM_model.score(scaled_train_data_x,train_data_y)\n",
    "test_score_SVM = SVM_model.score(scaled_test_data_x,test_data_y)\n",
    "accuracy_SVM = accuracy_score(test_data_y,SVM_prediction)\n",
    "f1_score_SVM = f1_score(test_data_y, SVM_prediction)\n",
    "\n",
    "print(f\"Decision Tree Train Score : {train_score_SVM}\" )\n",
    "print(f\"Decision Tree Test Score : {test_score_SVM}\" )\n",
    "print(f\"Decision Tree Model Accuracy : {accuracy_SVM}\")\n",
    "print(f\"Decision Tree Model f1 score : {f1_score_SVM}\")\n",
    "print('\\n')\n",
    "\n",
    "# Print Report\n",
    "print(classification_report(test_data_y,SVM_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Logistic Regression': {'Train': train_score_LR, 'Test': test_score_LR, 'Accuracy': accuracy_LR, 'F1 Score': f1_score_LR},\n",
    "        'Decision Tree': {'Train': train_score_DT, 'Test': test_score_DT, 'Accuracy': accuracy_DT, 'F1 Score': f1_score_DT},\n",
    "        'Support Vector Machine': {'Train': train_score_SVM, 'Test': test_score_SVM, 'Accuracy': accuracy_SVM, 'F1 Score': f1_score_SVM}}\n",
    "df = pd.DataFrame(data)\n",
    "df = df.T\n",
    "df ['sum'] = df.sum(axis=1)\n",
    "df.sort_values('sum', ascending=False)[['Test','Train','Accuracy','F1 Score']].plot.bar() \n",
    "plt.ylabel('Score')\n",
    "plt.savefig('Comparison.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
